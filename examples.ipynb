{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4066fb9e",
   "metadata": {},
   "source": [
    "## Synthetic cases\n",
    "\n",
    "In this part, we show the implementation of the Quadratic- and Cosine-type rewards: \n",
    "\n",
    "- Quadratic reward: \n",
    "$$\n",
    "\\mathbb{E}[r^{i}_{t,k}] = \\mathbf{x}_{t,k}^\\top \\mathbf{A}_i^\\top \\mathbf{A}_i \\mathbf{x}_{t,k}, \\forall i\\in[m],\n",
    "$$\n",
    "\n",
    "- Cosine reward: \n",
    "$$\n",
    "\\mathbb{E}[r^{i}_{t,k}] = \\cos\\left(3\\,\\mathbf{x}_{t,k}^\\top \\boldsymbol{\\theta}^*_i\\right), \\forall i\\in[m],\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c33f56f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bcbb971",
   "metadata": {},
   "outputs": [],
   "source": [
    "from environments import moContextMABSimulator\n",
    "\n",
    "# Quadratic\n",
    "class quadEnv(moContextMABSimulator): \n",
    "    def __init__(self, num_arm = None, num_dim = None, num_obj = None, arm_context = None, obj_preference = 'scalarization', sclarization_type='weighted_sum', opt_type = 'max', vary_context = False, noise_var = 0.1):\n",
    "        super().__init__(num_arm, num_dim, num_obj, arm_context, obj_preference, sclarization_type, opt_type, vary_context, noise_var)\n",
    "\n",
    "    def reset(self, num_arm: int = None, num_dim: int = None, num_obj: int = None, noise_var: float = None, seed: int = None, verbose: bool = False) -> None:\n",
    "        self.quad_A = [np.random.normal(size=(self.d,self.d)) for _ in range(self.m)]\n",
    "        return super().reset(num_arm, num_dim, num_obj, noise_var, seed, verbose)\n",
    "    \n",
    "    def _sample_context(self):\n",
    "        self.A = np.random.rand(self.K, self.d)\n",
    "    \n",
    "    def _eval_expected_reward(self, arm):\n",
    "        return np.hstack([arm @ self.quad_A[i].T @ self.quad_A[i] @ arm.T for i in range(self.m)])\n",
    "\n",
    "# Cosine\n",
    "class cosiEnv(moContextMABSimulator): \n",
    "    def __init__(self, num_arm = None, num_dim = None, num_obj = None, arm_context = None, obj_preference = 'scalarization', sclarization_type='weighted_sum', opt_type = 'max', vary_context = False, noise_var = 0.1):\n",
    "        super().__init__(num_arm, num_dim, num_obj, arm_context, obj_preference, sclarization_type, opt_type, vary_context, noise_var)\n",
    "        \n",
    "    def reset(self, num_arm: int = None, num_dim: int = None, num_obj: int = None, noise_var: float = None, seed: int = None, verbose: bool = False) -> None:\n",
    "        self.cos_theta = [np.random.uniform(size=(self.d, )) for _ in range(self.m)]\n",
    "        return super().reset(num_arm, num_dim, num_obj, noise_var, seed, verbose)\n",
    "    \n",
    "    def _sample_context(self):\n",
    "        self.A = np.random.rand(self.K, self.d)\n",
    "    \n",
    "    def _eval_expected_reward(self, arm):\n",
    "        return np.hstack([np.cos(arm @ self.cos_theta[i]) for i in range(self.m)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b61b72b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Parameters**\n",
    "\n",
    "# number of objectives\n",
    "m = 2\n",
    "# number of dimension of the context\n",
    "d = 10\n",
    "# number of arms\n",
    "K = 10\n",
    "# number of rounds\n",
    "T = 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b504d4",
   "metadata": {},
   "source": [
    "- Define the agent and the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b33f05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import MONeural\n",
    "\n",
    "# learning agents\n",
    "mon_ucb = MONeural(\n",
    "    num_arm=K, \n",
    "    num_dim=d, \n",
    "    num_obj=m,\n",
    "    opt_type='min',\n",
    "    style='ucb', # method type: \"ucb\" or \"ts\"\n",
    "    lamda=1., \n",
    "    delta=.05, \n",
    "    hidden_size=256, \n",
    "    hidden_layer=1, \n",
    "    rho=0.05, # hyperparameter for exploration\n",
    "    lr=1e-2, \n",
    ")\n",
    "\n",
    "# simulation environment\n",
    "env = quadEnv(\n",
    "    num_arm=K, \n",
    "    num_dim=d, \n",
    "    num_obj=m,\n",
    "    vary_context=True,\n",
    "    opt_type='min',\n",
    "    noise_var=0.1, # variance of the Gaussian noise\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc10237",
   "metadata": {},
   "source": [
    "- Define the distribution of the preference vector (uniform distributed correlates to Pareto optimality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcb4a053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runif_in_simplex(n):\n",
    "  ''' Return uniformly random vector in the n-simplex '''\n",
    "  k = np.random.exponential(scale=1.0, size=n)\n",
    "  return k / sum(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "beb9fcc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round: 0, instantaneous regret: 1.3743, cumulative regret: 0.0000.\n",
      "Round: 100, instantaneous regret: 0.0000, cumulative regret: 411.8753.\n",
      "Round: 200, instantaneous regret: 0.0000, cumulative regret: 443.4100.\n",
      "Round: 300, instantaneous regret: 0.0000, cumulative regret: 447.6687.\n",
      "Round: 400, instantaneous regret: 0.0000, cumulative regret: 458.7001.\n",
      "Round: 500, instantaneous regret: 0.0000, cumulative regret: 464.3926.\n",
      "Round: 600, instantaneous regret: 0.0000, cumulative regret: 465.0309.\n",
      "Round: 700, instantaneous regret: 0.0000, cumulative regret: 465.2262.\n",
      "Round: 800, instantaneous regret: 0.0000, cumulative regret: 466.5677.\n",
      "Round: 900, instantaneous regret: 0.0000, cumulative regret: 473.3409.\n",
      "Round: 1000, instantaneous regret: 0.0000, cumulative regret: 473.7176.\n",
      "Round: 1100, instantaneous regret: 0.0000, cumulative regret: 474.6320.\n",
      "Round: 1200, instantaneous regret: 0.0000, cumulative regret: 476.4915.\n",
      "Round: 1300, instantaneous regret: 0.0000, cumulative regret: 476.9105.\n",
      "Round: 1400, instantaneous regret: 0.0000, cumulative regret: 477.9061.\n",
      "Round: 1500, instantaneous regret: 0.0000, cumulative regret: 478.5795.\n",
      "Round: 1600, instantaneous regret: 0.0000, cumulative regret: 479.6568.\n",
      "Round: 1700, instantaneous regret: 0.0000, cumulative regret: 480.3533.\n",
      "Round: 1800, instantaneous regret: 0.0000, cumulative regret: 480.9718.\n",
      "Round: 1900, instantaneous regret: 0.0000, cumulative regret: 481.2963.\n",
      "Round: 2000, instantaneous regret: 0.0000, cumulative regret: 481.2963.\n",
      "Round: 2100, instantaneous regret: 0.0000, cumulative regret: 481.2963.\n",
      "Round: 2200, instantaneous regret: 0.0000, cumulative regret: 482.7616.\n",
      "Round: 2300, instantaneous regret: 0.0000, cumulative regret: 482.7616.\n",
      "Round: 2400, instantaneous regret: 0.0000, cumulative regret: 482.9502.\n",
      "Round: 2500, instantaneous regret: 0.0000, cumulative regret: 482.9502.\n",
      "Round: 2600, instantaneous regret: 0.0000, cumulative regret: 483.4133.\n",
      "Round: 2700, instantaneous regret: 0.0000, cumulative regret: 484.9400.\n",
      "Round: 2800, instantaneous regret: 0.0000, cumulative regret: 484.9853.\n",
      "Round: 2900, instantaneous regret: 0.0000, cumulative regret: 484.9853.\n"
     ]
    }
   ],
   "source": [
    "# reset the agent and environment\n",
    "mon_ucb.reset()\n",
    "env.reset()\n",
    "\n",
    "tot_reg = 0\n",
    "for t in range(T): \n",
    "    # sample preference vector \n",
    "    weight_vector = runif_in_simplex(m)\n",
    "    # observe the context\n",
    "    X = env.observe_context()\n",
    "    # select an arm by the agent\n",
    "    a_t = mon_ucb.take_action(context=X, weight_vector=weight_vector)\n",
    "    # obtian the reward (and the regret (for evaluating the performance))\n",
    "    r_t = env.get_reward(arm=a_t)\n",
    "    reg_t = env.get_regret(arm=a_t, weight_vector=weight_vector).item()\n",
    "    # update the agent\n",
    "    mon_ucb.update(info=(a_t, r_t, X[a_t]))\n",
    "    # print the cumulative regret\n",
    "    if t%100 == 0: print(f\"Round: {t:d}, instantaneous regret: {reg_t:.4f}, cumulative regret: {tot_reg:.4f}.\")\n",
    "    tot_reg += reg_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66cb380",
   "metadata": {},
   "source": [
    "# MOO Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81a7ffca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_problem\n",
    "\n",
    "p_name = \"DTLZ2\"\n",
    "p = get_problem(p_name)\n",
    "\n",
    "d = p.n_dim\n",
    "m = p.n_obj\n",
    "\n",
    "class mooEnv(moContextMABSimulator): \n",
    "    def _sample_context(self):\n",
    "        self.A = np.random.rand(self.K, self.d)\n",
    "    \n",
    "    def _eval_expected_reward(self, arm):\n",
    "        return p.evaluate(torch.tensor(arm)).to('cpu').numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81eaebbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of arms\n",
    "K = 20\n",
    "# number of rounds\n",
    "T = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1023220",
   "metadata": {},
   "outputs": [],
   "source": [
    "mon_ts = MONeural(\n",
    "    opt_type='min',\n",
    "    style='ts',\n",
    "    lamda=1.,\n",
    "    delta=.05, \n",
    "    num_arm=K, \n",
    "    num_dim=d, \n",
    "    num_obj=m,\n",
    "    hidden_size=256, \n",
    "    hidden_layer=2,\n",
    "    rho=0.01, \n",
    ")\n",
    "\n",
    "env = mooEnv(\n",
    "    num_arm=K, \n",
    "    num_dim=d, \n",
    "    num_obj=m, \n",
    "    vary_context=1, \n",
    "    opt_type='min',\n",
    "    noise_var=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c650f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round: 0, instantaneous regret: 0.1523, cumulative regret: 0.0000.\n",
      "Round: 100, instantaneous regret: 0.0000, cumulative regret: 18.3629.\n",
      "Round: 200, instantaneous regret: 0.0321, cumulative regret: 22.2620.\n",
      "Round: 300, instantaneous regret: 0.0000, cumulative regret: 23.3036.\n",
      "Round: 400, instantaneous regret: 0.0000, cumulative regret: 24.2018.\n",
      "Round: 500, instantaneous regret: 0.0000, cumulative regret: 24.7917.\n",
      "Round: 600, instantaneous regret: 0.0000, cumulative regret: 25.2387.\n",
      "Round: 700, instantaneous regret: 0.0181, cumulative regret: 25.6296.\n",
      "Round: 800, instantaneous regret: 0.0000, cumulative regret: 26.0412.\n",
      "Round: 900, instantaneous regret: 0.0000, cumulative regret: 26.5372.\n"
     ]
    }
   ],
   "source": [
    "alg = mon_ts\n",
    "# reset the agent and environment\n",
    "alg.reset()\n",
    "env.reset()\n",
    "\n",
    "tot_reg = 0\n",
    "for t in range(T): \n",
    "    # sample preference vector \n",
    "    weight_vector = runif_in_simplex(m)\n",
    "    # observe the context\n",
    "    X = env.observe_context()\n",
    "    # select an arm by the agent\n",
    "    a_t = alg.take_action(context=X, weight_vector=weight_vector)\n",
    "    # obtian the reward (and the regret (for evaluating the performance))\n",
    "    r_t = env.get_reward(arm=a_t)\n",
    "    reg_t = env.get_regret(arm=a_t, weight_vector=weight_vector).item()\n",
    "    # update the agent\n",
    "    alg.update(info=(a_t, r_t, X[a_t]))\n",
    "    # print the cumulative regret\n",
    "    if t%100 == 0: print(f\"Round: {t:d}, instantaneous regret: {reg_t:.4f}, cumulative regret: {tot_reg:.4f}.\")\n",
    "    tot_reg += reg_t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML3_12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
